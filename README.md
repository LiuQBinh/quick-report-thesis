# A. Giá»›i thiá»‡u
## A.1. KhÃ³ khÄƒn vÃ  Nhu cáº§u Tá»‘i Æ°u Há»‡ thá»‘ng Há»— trá»£ Sinh viÃªn
Hiá»‡n nay, táº¡i cÃ¡c trÆ°á»ng Ä‘áº¡i há»c, dá»‹ch vá»¥ há»— trá»£ sinh viÃªn Ä‘Ã³ng vai trÃ² quan trá»ng trong viá»‡c xá»­ lÃ½ cÃ¡c tháº¯c máº¯c vÃ  Ä‘Ã¡p á»©ng yÃªu cáº§u vá» hÃ nh chÃ­nh, há»c thuáº­t cÅ©ng nhÆ° cÃ¡c váº¥n Ä‘á» cÃ¡ nhÃ¢n cá»§a sinh viÃªn. Nhá»¯ng dá»‹ch vá»¥ nÃ y lÃ  cáº§u ná»‘i giá»¯a sinh viÃªn vÃ  nhÃ  trÆ°á»ng, há»— trá»£ Ä‘Äƒng kÃ½ mÃ´n há»c, thÃ´ng tin há»c bá»•ng, há»c phÃ­, thÃ´ng tin tá»‘t nghiá»‡p, Ä‘iá»u chá»‰nh khÃ³a há»c vÃ  nhiá»u yÃªu cáº§u khÃ¡c.

Há»‡ thá»‘ng há»— trá»£ hoáº¡t Ä‘á»™ng nhÆ° má»™t kÃªnh trÃ² chuyá»‡n giá»¯a sinh viÃªn vÃ  Ä‘á»™i ngÅ© há»— trá»£. Khi sinh viÃªn cáº§n há»— trá»£, há» sáº½ táº¡o cÃ¢u há»i hoáº·c yÃªu cáº§u vÃ  gá»­i Ä‘áº¿n há»‡ thá»‘ng. Há»‡ thá»‘ng sau Ä‘Ã³ chuyá»ƒn cÃ¢u há»i Ä‘áº¿n ngÆ°á»i phá»¥ trÃ¡ch cÃ³ chuyÃªn mÃ´n phÃ¹ há»£p. Sau khi tiáº¿p nháº­n, chuyÃªn viÃªn sáº½ pháº£n há»“i má»™t cÃ¡ch chÃ­nh xÃ¡c Ä‘á»ƒ giáº£i quyáº¿t váº¥n Ä‘á» cá»§a sinh viÃªn vÃ  gá»­i láº¡i qua há»‡ thá»‘ng. Quy trÃ¬nh nÃ y Ä‘Æ°á»£c minh há»a trong sÆ¡ Ä‘á»“ HÃ¬nh 1.

<sÆ¡ Ä‘á»“ - HÃ¬nh 1>

Táº¡i há»‡ thá»‘ng BKSI - kÃªnh há»— trá»£ sinh viÃªn cá»§a TrÆ°á»ng Äáº¡i há»c BÃ¡ch Khoa, Äáº¡i há»c Quá»‘c gia TP.HCM, sinh viÃªn cÃ³ thá»ƒ Ä‘áº·t cÃ¢u há»i báº±ng cÃ¡ch chá»n loáº¡i yÃªu cáº§u, nháº­p chá»§ Ä‘á» vÃ  ná»™i dung, sau Ä‘Ã³ gá»­i Ä‘i (HÃ¬nh 2).

<Giao diá»‡n - hÃ¬nh 2>

Nhá»¯ng yÃªu cáº§u cá»§a sinh viÃªn sáº½ Ä‘Æ°á»£c xá»­ lÃ½ bá»Ÿi cÃ¡c cÃ¡n bá»™ phá»¥ trÃ¡ch trong trÆ°á»ng. Giao diá»‡n cho cÃ¡n bá»™ tráº£ lá»i sinh viÃªn Ä‘Æ°á»£c mÃ´ táº£ á»Ÿ HÃ¬nh 3. Khi sinh viÃªn gá»­i yÃªu cáº§u, há»‡ thá»‘ng táº¡o má»™t â€œticketâ€ â€“ luá»“ng thÃ´ng tin cho phÃ©p hai bÃªn trao Ä‘á»•i. Náº¿u sinh viÃªn cÃ³ thÃªm tháº¯c máº¯c hoáº·c chÆ°a hÃ i lÃ²ng vá»›i cÃ¢u tráº£ lá»i, há» cÃ³ thá»ƒ tiáº¿p tá»¥c pháº£n há»“i trong ticket nÃ y.

CÃ¡n bá»™ há»— trá»£ cáº§n Ä‘á»c hiá»ƒu ká»¹ cÃ¢u há»i tá»« sinh viÃªn vÃ  cÃ³ kiáº¿n thá»©c sÃ¢u vá» quy Ä‘á»‹nh cÅ©ng nhÆ° cÃ¡c nghiá»‡p vá»¥ cá»§a trÆ°á»ng Ä‘á»ƒ tráº£ lá»i chÃ­nh xÃ¡c. HÃ¬nh 4 lÃ  giao diá»‡n cá»§a cÃ¡n bá»™ khi tráº£ lá»i cÃ¢u há»i cá»§a sinh viÃªn.

<HÃ¬nh 3, giao diá»‡n tiáº¿p nháº­n cÃ¢u há»i> <HÃ¬nh 4, cÃ¡n bá»™ nháº­p ná»™i dung tráº£ lá»i>

Viá»‡c nhá»› háº¿t cÃ¡c quy Ä‘á»‹nh vÃ  nghiá»‡p vá»¥ cá»§a nhÃ  trÆ°á»ng lÃ  khÃ´ng dá»… dÃ ng. CÃ¡c chuyÃªn viÃªn cÃ³ thá»ƒ cáº§n pháº£i tra cá»©u láº¡i tÃ i liá»‡u Ä‘á»ƒ Ä‘áº£m báº£o cÃ¢u tráº£ lá»i chÃ­nh xÃ¡c, tiÃªu tá»‘n khÃ´ng Ã­t thá»i gian vÃ  cÃ´ng sá»©c. HÆ¡n ná»¯a, cÃ¡c cÃ¢u há»i dÃ i, lan man cÃ³ thá»ƒ khiáº¿n chuyÃªn viÃªn khÃ³ hiá»ƒu rÃµ Ã½ Ä‘á»‹nh cá»§a sinh viÃªn, gÃ¢y lÃ£ng phÃ­ thá»i gian (HÃ¬nh 5). Viá»‡c xÃ¡c Ä‘á»‹nh rÃµ Ã½ Ä‘á»‹nh vÃ  thÃ´ng tin cáº§n thiáº¿t giÃºp cÃ¡n bá»™ tráº£ lá»i nhanh vÃ  hiá»‡u quáº£ hÆ¡n.

<HÃ¬nh 5 - cÃ¢u há»i dÃ i cá»§a sinh viÃªn>

Vá»›i sá»‘ lÆ°á»£ng yÃªu cáº§u lá»›n, viá»‡c pháº£n há»“i ká»‹p thá»i lÃ  thÃ¡ch thá»©c cho cÃ¡c trÆ°á»ng Ä‘áº¡i há»c. Nhiá»u trÆ°á»ng Ä‘ang dáº§n chuyá»ƒn dá»‹ch vá»¥ há»— trá»£ tá»« con ngÆ°á»i sang há»‡ thá»‘ng mÃ¡y tÃ­nh thÃ´ng minh nhÆ° trá»£ lÃ½ áº£o hay chatbot. Ná»n táº£ng cá»§a há»‡ thá»‘ng nÃ y lÃ  Ä‘á»“ thá»‹ tri thá»©c, giÃºp quáº£n lÃ½ kiáº¿n thá»©c vá» nghiá»‡p vá»¥ vÃ  nháº­n diá»‡n Ã½ Ä‘á»‹nh cá»§a sinh viÃªn Ä‘á»ƒ há»— trá»£ hiá»‡u quáº£ hÆ¡n.
## A.2. Vai trÃ² cá»§a Trá»£ lÃ½ áº¢o vÃ  Äá»“ thá»‹ Tri thá»©c trong NÃ¢ng cao Hiá»‡u quáº£ Há»— trá»£ Sinh viÃªn
Trong bá»‘i cáº£nh hiá»‡n Ä‘áº¡i, trá»£ lÃ½ áº£o Ä‘ang trá»Ÿ nÃªn phá»• biáº¿n vÃ  ngÃ y cÃ ng Ä‘Ã³ng vai trÃ² quan trá»ng, do Ä‘Ã³ viá»‡c nÃ¢ng cao hiá»‡u quáº£ cá»§a chÃºng lÃ  Ä‘iá»u cáº§n thiáº¿t. Má»™t trong nhá»¯ng yáº¿u tá»‘ quyáº¿t Ä‘á»‹nh sá»± thÃ nh cÃ´ng cá»§a cÃ¡c trá»£ lÃ½ áº£o chÃ­nh lÃ  kháº£ nÄƒng hiá»ƒu vÃ  Ä‘Ã¡p á»©ng Ä‘Ãºng yÃªu cáº§u cá»§a ngÆ°á»i dÃ¹ng. Trong xá»­ lÃ½ ngÃ´n ngá»¯, yÃªu cáº§u cá»§a ngÆ°á»i dÃ¹ng cÃ³ thá»ƒ Ä‘Æ°á»£c tÃ³m táº¯t dÆ°á»›i dáº¡ng â€œkhung ngá»¯ nghÄ©aâ€ (semantic frame), bao gá»“m Ã½ Ä‘á»‹nh vÃ  cÃ¡c thÃ´ng tin cáº§n thiáº¿t khÃ¡c. Trong há»‡ thá»‘ng há»— trá»£ sinh viÃªn, viá»‡c nháº­n diá»‡n Ã½ Ä‘á»‹nh cá»§a ngÆ°á»i dÃ¹ng khÃ´ng chá»‰ quan trá»ng cho tá»± Ä‘á»™ng hÃ³a mÃ  cÃ²n giÃºp cÃ¡n bá»™ xá»­ lÃ½ cÃ¢u há»i dá»… dÃ ng. Thay vÃ¬ Ä‘á»c toÃ n bá»™ cÃ¢u há»i dÃ i, cÃ¡n bá»™ chá»‰ cáº§n xem thÃ´ng tin tÃ³m lÆ°á»£c trong khung ngá»¯ nghÄ©a Ä‘á»ƒ hiá»ƒu váº¥n Ä‘á» vÃ  pháº£n há»“i nhanh chÃ³ng.

Má»™t thÃ¡ch thá»©c khÃ¡c cá»§a há»‡ thá»‘ng há»— trá»£ sinh viÃªn lÃ  táº¡o ra ná»n táº£ng vá»«a lÆ°u trá»¯, vá»«a dá»… dÃ ng tÃ¬m kiáº¿m thÃ´ng tin. Äá»“ thá»‹ tri thá»©c hiá»‡n nay lÃ  cÃ´ng nghá»‡ phÃ¹ há»£p Ä‘á»ƒ giáº£i quyáº¿t nhu cáº§u nÃ y, láº¥y Ã½ tÆ°á»Ÿng tá»« Semantic Web lÃ  "tá»• chá»©c" vÃ  "gáº¯n káº¿t" dá»¯ liá»‡u trÃªn web má»™t cÃ¡ch cÃ³ Ã½ nghÄ©a hÆ¡n, dá»±a trÃªn cÃ¡c tiÃªu chuáº©n vÃ  Ä‘á»‹nh dáº¡ng chung, Ä‘á»ƒ mÃ¡y tÃ­nh cÃ³ thá»ƒ â€œhiá»ƒuâ€ vÃ  liÃªn káº¿t cÃ¡c dá»¯ liá»‡u nÃ y láº¡i vá»›i nhau. Trong giÃ¡o dá»¥c, Ä‘á»“ thá»‹ tri thá»©c cÃ³ thá»ƒ tÃ­ch há»£p thÃ´ng tin tá»« cÃ¡c cÆ¡ sá»Ÿ dá»¯ liá»‡u khÃ¡c nhau nhÆ° há»“ sÆ¡ sinh viÃªn, khÃ³a há»c, quy Ä‘á»‹nh trÆ°á»ng há»c vÃ o má»™t Ä‘á»‹nh dáº¡ng chung. CÃ¡c chatbot vÃ  há»‡ thá»‘ng há»i Ä‘Ã¡p Ä‘ang sá»­ dá»¥ng Ä‘á»“ thá»‹ tri thá»©c phá»• biáº¿n, vÃ¬ nÃ³ giÃºp truy xuáº¥t thÃ´ng tin chÃ­nh xÃ¡c hÆ¡n, giáº£m thiá»ƒu tÃ¬nh tráº¡ng táº¡o ra cÃ¡c thÃ´ng tin sai lá»‡ch á»Ÿ cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n (LLM).

Trong há»‡ thá»‘ng há»— trá»£ sinh viÃªn, Ä‘á»“ thá»‹ tri thá»©c khÃ´ng chá»‰ chá»©a kiáº¿n thá»©c nghiá»‡p vá»¥ vÃ  quy Ä‘á»‹nh cá»§a trÆ°á»ng mÃ  cÃ²n lÆ°u trá»¯ thÃ´ng tin vá» Ã½ Ä‘á»‹nh cá»§a sinh viÃªn. Äiá»u nÃ y giÃºp há»‡ thá»‘ng suy luáº­n vÃ  pháº£n há»“i hiá»‡u quáº£ hÆ¡n. VÃ­ dá»¥, khi sinh viÃªn muá»‘n Ä‘Äƒng kÃ½ luáº­n vÄƒn tá»‘t nghiá»‡p, há»‡ thá»‘ng cÃ³ thá»ƒ sá»­ dá»¥ng dá»¯ liá»‡u cÃ³ trong Ä‘á»“ thá»‹ tri thá»©c nhÆ° Ä‘iá»ƒm anh vÄƒn vÃ  quy cháº¿ trÆ°á»ng Ä‘á»ƒ xem sinh viÃªn cÃ³ Ä‘á»§ Ä‘iá»u kiá»‡n Ä‘Äƒng kÃ½ hay khÃ´ng, vÃ  Ä‘Æ°a ra pháº£n há»“i thÃ­ch há»£p.

Viá»‡c tÃ­ch há»£p khung ngá»¯ nghÄ©a (gá»“m Ã½ Ä‘á»‹nh vÃ  cÃ¡c thÃ´ng tin liÃªn quan) vÃ o Ä‘á»“ thá»‹ tri thá»©c giÃºp há»‡ thá»‘ng hoáº¡t Ä‘á»™ng tá»‘i Æ°u hÆ¡n. Má»™t bÃ i nghiÃªn cá»©u trÆ°á»›c Ä‘Ã¢y táº¡i HCMUT Ä‘Ã£ Ä‘á» xuáº¥t cáº¥u trÃºc Ä‘á»“ thá»‹ tri thá»©c tÃ­ch há»£p Ã½ Ä‘á»‹nh dÆ°á»›i dáº¡ng cÃ¡c thá»±c thá»ƒ vÃ  cÃ¡c má»‘i quan há»‡ liÃªn quan. Tiáº¿p ná»‘i Ã½ tÆ°á»Ÿng nÃ y, nhÃ³m chÃºng tÃ´i xÃ¢y dá»±ng Ä‘á»“ thá»‹ tri thá»©c xem cÃ¡c Ã½ Ä‘á»‹nh lÃ  má»‘i quan há»‡ giá»¯a cÃ¡c thá»±c thá»ƒ trong trÆ°á»ng. HÃ¬nh 6 minh há»a cáº¥u trÃºc cá»§a Ä‘á»“ thá»‹ tri thá»©c nÃ y, trong Ä‘Ã³ cÃ¡c khá»‘i hÃ¬nh chá»¯ nháº­t cáº¡nh trÃ²n biá»ƒu thá»‹ Ã½ Ä‘á»‹nh, khá»‘i hÃ¬nh bÃ¬nh hÃ nh biá»ƒu thá»‹ cÃ¡c quy Ä‘á»‹nh, vÃ  cÃ¡c khá»‘i oval biá»ƒu thá»‹ cÃ¡c thá»±c thá»ƒ khÃ¡c trong há»‡ thá»‘ng.

<HÃ¬nh 6: Äá»“ thá»‹ tri thá»©c bao gá»“m Ã½ Ä‘á»‹nh trong lÄ©nh vá»±c giÃ¡o dá»¥c [4]>

XÃ¢y dá»±ng Ä‘á»“ thá»‹ tri thá»©c lÃ  quÃ¡ trÃ¬nh quan trá»ng Ä‘á»ƒ tá»• chá»©c thÃ´ng tin vÃ  náº¯m báº¯t cÃ¡c má»‘i liÃªn há»‡ giá»¯a cÃ¡c thá»±c thá»ƒ trong má»™t lÄ©nh vá»±c cá»¥ thá»ƒ. Trong Ä‘Ã³, nhiá»‡m vá»¥ khÃ¡m phÃ¡ vÃ  rÃºt trÃ­ch quan há»‡ Ä‘Ã³ng vai trÃ² cá»‘t lÃµi, giÃºp táº¡o nÃªn cáº¥u trÃºc káº¿t ná»‘i logic giá»¯a cÃ¡c thá»±c thá»ƒ, tá»« Ä‘Ã³ nÃ¢ng cao kháº£ nÄƒng truy xuáº¥t vÃ  sá»­ dá»¥ng thÃ´ng tin.

KhÃ¡m phÃ¡ vÃ  rÃºt trÃ­ch quan há»‡ tá»« dá»¯ liá»‡u thÃ´ khÃ´ng chá»‰ Ä‘Ã²i há»i kháº£ nÄƒng xÃ¡c Ä‘á»‹nh Ä‘Ãºng cÃ¡c thá»±c thá»ƒ mÃ  cÃ²n cáº§n xÃ¡c Ä‘á»‹nh chÃ­nh xÃ¡c báº£n cháº¥t cá»§a má»‘i quan há»‡ giá»¯a chÃºng. CÃ¡c má»‘i quan há»‡ nÃ y cÃ³ thá»ƒ lÃ  cÃ¡c liÃªn káº¿t trá»±c tiáº¿p (nhÆ° giá»¯a sinh viÃªn vÃ  khÃ³a há»c Ä‘Ã£ Ä‘Äƒng kÃ½) hoáº·c cÃ¡c má»‘i quan há»‡ phá»©c táº¡p hÆ¡n, liÃªn quan Ä‘áº¿n quy Ä‘á»‹nh, quy cháº¿ vÃ  Ä‘iá»u kiá»‡n cá»¥ thá»ƒ. QuÃ¡ trÃ¬nh rÃºt trÃ­ch quan há»‡ chÃ­nh xÃ¡c giÃºp há»‡ thá»‘ng cÃ³ thá»ƒ suy luáº­n vÃ  cung cáº¥p thÃ´ng tin phÃ¹ há»£p dá»±a trÃªn bá»‘i cáº£nh, giáº£m thiá»ƒu thá»i gian xá»­ lÃ½ vÃ  nÃ¢ng cao Ä‘á»™ chÃ­nh xÃ¡c trong pháº£n há»“i.

PhÆ°Æ¡ng phÃ¡p rÃºt trÃ­ch quan há»‡ hiá»‡n Ä‘áº¡i sá»­ dá»¥ng cÃ¡c ká»¹ thuáº­t xá»­ lÃ½ ngÃ´n ngá»¯ tá»± nhiÃªn vÃ  há»c mÃ¡y Ä‘á»ƒ phÃ¡t hiá»‡n ra cÃ¡c kiá»ƒu quan há»‡ tiá»m nÄƒng giá»¯a cÃ¡c thá»±c thá»ƒ, sau Ä‘Ã³ tÃ­ch há»£p cÃ¡c quan há»‡ nÃ y vÃ o cáº¥u trÃºc Ä‘á»“ thá»‹ tri thá»©c. CÃ¡c há»‡ thá»‘ng nÃ y cÃ³ kháº£ nÄƒng há»c vÃ  má»Ÿ rá»™ng dá»±a trÃªn dá»¯ liá»‡u Ä‘áº§u vÃ o má»›i, giÃºp Ä‘á»“ thá»‹ tri thá»©c liÃªn tá»¥c phÃ¡t triá»ƒn vÃ  pháº£n Ã¡nh chÃ­nh xÃ¡c má»‘i quan há»‡ thá»±c táº¿.
## A.3. Má»¥c tiÃªu nghiÃªn cá»©u
Dá»±a trÃªn váº¥n Ä‘á» Ä‘Ã£ trÃ¬nh bÃ y, nhÃ³m nghiÃªn cá»©u Ä‘áº·t ra hai má»¥c tiÃªu chÃ­nh:
- XÃ¢y dá»±ng má»™t framework nháº±m khÃ¡m phÃ¡ cÃ¡c má»‘i quan há»‡ má»›i tá»« cÃ¡c táº­p thá»±c thá»ƒ vÃ  má»‘i quan há»‡ Ä‘Ã£ biáº¿t.
- Äá» xuáº¥t phÆ°Æ¡ng phÃ¡p cho nhiá»‡m vá»¥ "khÃ¡m phÃ¡ quan há»‡".
## A.4 Pháº¡m vi Ä‘á» tÃ i
## A.5 Cáº¥u trÃºc BÃ i viáº¿t
Ná»™i dung cÃ²n láº¡i cá»§a bÃ¡o cÃ¡o sáº½ Ä‘Æ°á»£c trÃ¬nh bÃ y theo cáº¥u trÃºc dÆ°á»›i Ä‘Ã¢y:

### ChÆ°Æ¡ng 2: CÆ¡ sá»Ÿ lÃ½ thuyáº¿t
ChÆ°Æ¡ng nÃ y sáº½ trÃ¬nh bÃ y cÃ¡c khÃ¡i niá»‡m lÃ½ thuyáº¿t ná»n táº£ng mÃ  nhÃ³m nghiÃªn cá»©u Ã¡p dá»¥ng cho Ä‘á» tÃ i. Ná»™i dung bao gá»“m cÃ¡c kiáº¿n thá»©c liÃªn quan Ä‘áº¿n Äá»“ thá»‹ tri thá»©c (KG), Äá»c hiá»ƒu mÃ¡y (MRC), Attention, cÃ¹ng vá»›i nhiá»u váº¥n Ä‘á» khÃ¡c trong lÄ©nh vá»±c Há»c mÃ¡y (ML) vÃ  Xá»­ lÃ½ ngÃ´n ngá»¯ tá»± nhiÃªn (NLP). CÃ¡c kiáº¿n trÃºc mÃ´ hÃ¬nh sá»­ dá»¥ng cÅ©ng sáº½ Ä‘Æ°á»£c giá»›i thiá»‡u trong chÆ°Æ¡ng nÃ y.

### ChÆ°Æ¡ng 3: CÃ¡c cÃ´ng trÃ¬nh liÃªn quan
ChÆ°Æ¡ng nÃ y cung cáº¥p cÃ¡i nhÃ¬n tá»•ng quan vá» cÃ¡c nghiÃªn cá»©u trÆ°á»›c Ä‘Ã¢y liÃªn quan Ä‘áº¿n cÃ¡c nhiá»‡m vá»¥ trong Ä‘á» tÃ i, bao gá»“m XÃ¢y dá»±ng Ä‘á»“ thá»‹ tri thá»©c (KG), Äá»c hiá»ƒu mÃ¡y (MRC), MÃ´ hÃ¬nh ngÃ´n ngá»¯ (LM). NhÃ³m sáº½ táº­p trung vÃ o nhá»¯ng nghiÃªn cá»©u trong lÄ©nh vá»±c giÃ¡o dá»¥c vÃ  ngÃ´n ngá»¯ Viá»‡t, tá»« Ä‘Ã³ Ä‘á» xuáº¥t phÆ°Æ¡ng phÃ¡p vÃ  mÃ´ hÃ¬nh dá»±a trÃªn cÃ¡c nghiÃªn cá»©u nÃ y.

### ChÆ°Æ¡ng 4: PhÆ°Æ¡ng phÃ¡p
ChÆ°Æ¡ng nÃ y sáº½ mÃ´ táº£ chi tiáº¿t vá» framework mÃ  nhÃ³m Ä‘Ã£ phÃ¡t triá»ƒn, cÅ©ng nhÆ° phÆ°Æ¡ng phÃ¡p nghiÃªn cá»©u, bao gá»“m tá»•ng thá»ƒ quy trÃ¬nh vÃ  tá»«ng bÆ°á»›c xá»­ lÃ½ cá»¥ thá»ƒ.

### ChÆ°Æ¡ng 5: ThÃ­ nghiá»‡m
Trong chÆ°Æ¡ng nÃ y, nhÃ³m sáº½ trÃ¬nh bÃ y cÃ¡c thÃ´ng tin chi tiáº¿t vá» dá»¯ liá»‡u Ä‘Æ°á»£c sá»­ dá»¥ng, cÃ¡ch thá»©c thiáº¿t láº­p thÃ­ nghiá»‡m, vÃ  káº¿t quáº£ Ä‘Ã¡nh giÃ¡ cÃ¡c thÃ­ nghiá»‡m nÃ y. BÃªn cáº¡nh Ä‘Ã³, nhÃ³m cÅ©ng sáº½ tiáº¿n hÃ nh phÃ¢n tÃ­ch vÃ  Ä‘Ã¡nh giÃ¡ cÃ¡c káº¿t quáº£ thu Ä‘Æ°á»£c.

### ChÆ°Æ¡ng 6: Tá»•ng káº¿t
ChÆ°Æ¡ng nÃ y sáº½ tÃ³m táº¯t cÃ¡c káº¿t quáº£ Ä‘áº¡t Ä‘Æ°á»£c cá»§a nhÃ³m so vá»›i má»¥c tiÃªu ban Ä‘áº§u, bao gá»“m nhá»¯ng thÃ nh tá»±u Ä‘Ã£ hoÃ n thÃ nh vÃ  nhá»¯ng Ä‘iá»ƒm cáº§n cáº£i thiá»‡n. Äá»“ng thá»i, báº£ng phÃ¢n chia khá»‘i lÆ°á»£ng cÃ´ng viá»‡c cá»§a cÃ¡c thÃ nh viÃªn trong nhÃ³m cÅ©ng sáº½ Ä‘Æ°á»£c cÃ´ng bá»‘ trong chÆ°Æ¡ng nÃ y.
# B. CÆ¡ sá»Ÿ lÃ½ thuyáº¿t
## B.1. CÆ¡ cháº¿ Attention (â€œAttention is All You Needâ€)
CÆ¡ cháº¿ Attention (Sá»± chÃº Ã½) lÃ  má»™t khÃ¡i niá»‡m quan trá»ng trong lÄ©nh vá»±c há»c sÃ¢u, Ä‘áº·c biá»‡t trong cÃ¡c mÃ´ hÃ¬nh xá»­ lÃ½ ngÃ´n ngá»¯ tá»± nhiÃªn vÃ  thá»‹ giÃ¡c mÃ¡y. DÆ°á»›i Ä‘Ã¢y lÃ  giáº£i thÃ­ch chi tiáº¿t vá» cÃ¡ch hoáº¡t Ä‘á»™ng cá»§a cÆ¡ cháº¿ Attention:
### 1. KhÃ¡i niá»‡m chung
CÆ¡ cháº¿ Attention cho phÃ©p mÃ´ hÃ¬nh táº­p trung vÃ o nhá»¯ng pháº§n quan trá»ng cá»§a dá»¯ liá»‡u Ä‘áº§u vÃ o trong quÃ¡ trÃ¬nh xá»­ lÃ½ thÃ´ng tin. Thay vÃ¬ xem táº¥t cáº£ cÃ¡c Ä‘áº§u vÃ o lÃ  ngang báº±ng nhau, Attention giÃºp xÃ¡c Ä‘á»‹nh nhá»¯ng pháº§n nÃ o lÃ  quan trá»ng nháº¥t Ä‘á»ƒ Ä‘Æ°a ra dá»± Ä‘oÃ¡n chÃ­nh xÃ¡c hÆ¡n.

### 2. Cáº¥u trÃºc cÆ¡ báº£n
CÆ¡ cháº¿ Attention thÆ°á»ng bao gá»“m ba thÃ nh pháº§n chÃ­nh:
- Query (Q): LÃ  Ä‘áº§u vÃ o mÃ  mÃ´ hÃ¬nh muá»‘n tÃ¬m kiáº¿m thÃ´ng tin tÆ°Æ¡ng á»©ng tá»« cÃ¡c Ä‘áº§u vÃ o khÃ¡c.
- Key (K): LÃ  cÃ¡c Ä‘áº§u vÃ o (thÆ°á»ng lÃ  cÃ¡c pháº§n tá»­ trong chuá»—i Ä‘áº§u vÃ o) mÃ  mÃ´ hÃ¬nh sáº½ so sÃ¡nh vá»›i Query Ä‘á»ƒ xÃ¡c Ä‘á»‹nh má»©c Ä‘á»™ liÃªn quan.
- Value (V): LÃ  thÃ´ng tin mÃ  mÃ´ hÃ¬nh sáº½ thu tháº­p dá»±a trÃªn sá»± so sÃ¡nh giá»¯a Query vÃ  Key.
### 3. Quy trÃ¬nh hoáº¡t Ä‘á»™ng
Quy trÃ¬nh hoáº¡t Ä‘á»™ng cá»§a Attention thÆ°á»ng Ä‘Æ°á»£c mÃ´ táº£ qua cÃ¡c bÆ°á»›c sau:

- TÃ­nh toÃ¡n Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng: Äáº§u tiÃªn, mÃ´ hÃ¬nh tÃ­nh toÃ¡n Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng giá»¯a Query vÃ  má»—i Key. ThÃ´ng thÆ°á»ng, Ä‘iá»u nÃ y Ä‘Æ°á»£c thá»±c hiá»‡n thÃ´ng qua phÃ©p nhÃ¢n dot product hoáº·c cosine similarity.

- Ãp dá»¥ng hÃ m softmax: Sau khi tÃ­nh toÃ¡n Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng, mÃ´ hÃ¬nh Ã¡p dá»¥ng hÃ m softmax lÃªn cÃ¡c giÃ¡ trá»‹ Ä‘á»™ tÆ°Æ¡ng Ä‘á»“ng Ä‘á»ƒ biáº¿n chÃºng thÃ nh cÃ¡c xÃ¡c suáº¥t, cho phÃ©p mÃ´ hÃ¬nh xÃ¡c Ä‘á»‹nh má»©c Ä‘á»™ quan trá»ng cá»§a tá»«ng Key.

- Káº¿t há»£p cÃ¡c giÃ¡ trá»‹: Cuá»‘i cÃ¹ng, cÃ¡c xÃ¡c suáº¥t nÃ y Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ tÃ­nh toÃ¡n má»™t trá»ng sá»‘ cho cÃ¡c Value tÆ°Æ¡ng á»©ng. MÃ´ hÃ¬nh káº¿t há»£p cÃ¡c Value dá»±a trÃªn trá»ng sá»‘ nÃ y Ä‘á»ƒ táº¡o ra Ä‘áº§u ra cuá»‘i cÃ¹ng.

### 4. TÃ­nh toÃ¡n Attention
CÃ´ng thá»©c tÃ­nh Attention cÃ³ thá»ƒ Ä‘Æ°á»£c biá»ƒu diá»…n nhÆ° sau:

![alt text](/image.png)

Trong Ä‘Ã³:
- ğ‘„: Q lÃ  ma tráº­n Query.
- ğ¾: K lÃ  ma tráº­n Key.
- ğ‘‰: V lÃ  ma tráº­n Value.
- ğ‘‘ğ‘˜ lÃ  kÃ­ch thÆ°á»›c cá»§a Key, Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ Ä‘iá»u chá»‰nh Ä‘á»™ lá»›n cá»§a giÃ¡ trá»‹ dot product.

### 6. á»¨ng dá»¥ng
CÆ¡ cháº¿ Attention Ä‘Ã£ Ä‘Æ°á»£c Ã¡p dá»¥ng rá»™ng rÃ£i trong cÃ¡c mÃ´ hÃ¬nh nhÆ° Transformers, BERT, vÃ  nhiá»u mÃ´ hÃ¬nh há»c sÃ¢u khÃ¡c trong lÄ©nh vá»±c xá»­ lÃ½ ngÃ´n ngá»¯ tá»± nhiÃªn vÃ  thá»‹ giÃ¡c mÃ¡y, giÃºp cáº£i thiá»‡n Ä‘Ã¡ng ká»ƒ Ä‘á»™ chÃ­nh xÃ¡c vÃ  hiá»‡u suáº¥t cá»§a cÃ¡c mÃ´ hÃ¬nh nÃ y.
CÆ¡ cháº¿ Attention lÃ  má»™t trong nhá»¯ng phÃ¡t minh quan trá»ng nháº¥t trong há»c sÃ¢u, giÃºp cÃ¡c mÃ´ hÃ¬nh trá»Ÿ nÃªn máº¡nh máº½ hÆ¡n trong viá»‡c xá»­ lÃ½ vÃ  hiá»ƒu thÃ´ng tin phá»©c táº¡p. Nhá» cÃ³ Attention, mÃ´ hÃ¬nh cÃ³ kháº£ nÄƒng táº­p trung vÃ o nhá»¯ng pháº§n quan trá»ng nháº¥t cá»§a dá»¯ liá»‡u, tá»« Ä‘Ã³ cáº£i thiá»‡n hiá»‡u suáº¥t vÃ  Ä‘á»™ chÃ­nh xÃ¡c trong cÃ¡c nhiá»‡m vá»¥ khÃ¡c nhau.

Nguá»“n tham kháº£o: https://arxiv.org/pdf/1706.03762
## B.2. MÃ´ hÃ¬nh transformers
## B.3. Pre-trained language model (CÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ huáº¥n luyá»‡n trÆ°á»›c)
CÃ¡c mÃ´ hÃ¬nh huáº¥n luyá»‡n trÆ°á»›c lÃ  cÃ¡c máº¡ng nÆ¡-ron hoáº·c mÃ´ hÃ¬nh mÃ¡y há»c Ä‘Ã£ Ä‘Æ°á»£c Ä‘Ã o táº¡o trÃªn má»™t táº­p dá»¯ liá»‡u lá»›n, thÆ°á»ng Ä‘Ã²i há»i nhiá»u tÃ i nguyÃªn tÃ­nh toÃ¡n vÃ  thá»i gian. Nhá»¯ng mÃ´ hÃ¬nh nÃ y cÃ³ thá»ƒ Ä‘Æ°á»£c tinh chá»‰nh hoáº·c Ã¡p dá»¥ng trá»±c tiáº¿p cho cÃ¡c nhiá»‡m vá»¥ cá»¥ thá»ƒ, nhá» vÃ o viá»‡c khai thÃ¡c cÃ¡c Ä‘áº·c Ä‘iá»ƒm vÃ  máº«u Ä‘Ã£ há»c tá»« táº­p dá»¯ liá»‡u ban Ä‘áº§u. MÃ´ hÃ¬nh huáº¥n luyá»‡n trÆ°á»›c Ä‘áº·c biá»‡t cÃ³ giÃ¡ trá»‹ trong cÃ¡c trÆ°á»ng há»£p mÃ  viá»‡c Ä‘Ã o táº¡o mÃ´ hÃ¬nh tá»« Ä‘áº§u khÃ´ng kháº£ thi do háº¡n cháº¿ vá» dá»¯ liá»‡u, tÃ i nguyÃªn tÃ­nh toÃ¡n hoáº·c thá»i gian.

Má»™t sá»‘ mÃ´ hÃ¬nh huáº¥n luyá»‡n trÆ°á»›c ná»•i báº­t hiá»‡n nay nhÆ° BERT [12], GPT [52], vÃ  RoBERTa [30] Ä‘Ã³ng vai trÃ² quan trá»ng trong lÄ©nh vá»±c xá»­ lÃ½ ngÃ´n ngá»¯ tá»± nhiÃªn, nhá» vÃ o kháº£ nÄƒng náº¯m báº¯t vÃ  hiá»ƒu ngá»¯ cáº£nh cá»§a vÄƒn báº£n má»™t cÃ¡ch hiá»‡u quáº£.

BÃªn dÆ°á»›i lÃ  má»™t sá»‘ mÃ´ hÃ¬nh phá»• biáº¿n hiá»‡n nay.

### MÃ´ hÃ¬nh BERT
BERT (Bidirectional Encoder Representations from Transformers) lÃ  má»™t mÃ´ hÃ¬nh há»c sÃ¢u Ä‘Æ°á»£c xÃ¢y dá»±ng trÃªn kiáº¿n trÃºc Transformers, do nhÃ³m nghiÃªn cá»©u Google AI Language phÃ¡t triá»ƒn. MÃ´ hÃ¬nh nÃ y hoáº¡t Ä‘á»™ng nhÆ° má»™t cÃ´ng cá»¥ Ä‘a nÄƒng cho hÆ¡n 11 tÃ¡c vá»¥ ngÃ´n ngá»¯ phá»• biáº¿n.

TrÆ°á»›c Ä‘Ã¢y, cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ chá»‰ cÃ³ thá»ƒ xá»­ lÃ½ vÄƒn báº£n Ä‘áº§u vÃ o theo thá»© tá»± tuáº§n tá»± tá»« trÃ¡i sang pháº£i (left-to-right) hoáº·c tá»« pháº£i sang trÃ¡i (right-to-left), mÃ  khÃ´ng thá»ƒ thá»±c hiá»‡n Ä‘á»“ng thá»i cáº£ hai hÆ°á»›ng. Äiá»ƒm khÃ¡c biá»‡t cá»§a BERT lÃ  kháº£ nÄƒng Ä‘á»c vÄƒn báº£n tá»« cáº£ hai phÃ­a cÃ¹ng lÃºc, nhá» vÃ o thiáº¿t káº¿ cho phÃ©p tÃ­nh hai chiá»u. Sá»± phÃ¡t triá»ƒn cá»§a cÃ¡c mÃ´ hÃ¬nh Transformers Ä‘Ã£ lÃ m cho kháº£ nÄƒng nÃ y trá»Ÿ thÃ nh hiá»‡n thá»±c. BERT sá»­ dá»¥ng bá»™ mÃ£ hÃ³a Transformers Ä‘á»ƒ há»c cÃ¡c biá»ƒu diá»…n tá»« (text representations) tá»« má»™t táº­p dá»¯ liá»‡u khÃ´ng Ä‘Æ°á»£c gÃ¡n nhÃ£n.

Kiáº¿n trÃºc cá»§a BERT bao gá»“m hai giai Ä‘oáº¡n: huáº¥n luyá»‡n trÆ°á»›c (pre-training) vÃ  tinh chá»‰nh (fine-tuning).
![alt text](image-1.png)

Trong giai Ä‘oáº¡n huáº¥n luyá»‡n trÆ°á»›c (pre-training), mÃ´ hÃ¬nh thá»±c hiá»‡n hai nhiá»‡m vá»¥ chÃ­nh:

- Masked Language Modeling (MLM): Trong nhiá»‡m vá»¥ nÃ y, má»™t pháº§n dá»¯ liá»‡u sáº½ bá»‹ che giáº¥u vÃ  má»™t tá»· lá»‡ nháº¥t Ä‘á»‹nh cÃ¡c tá»« trong cÃ¢u sáº½ Ä‘Æ°á»£c thay tháº¿ báº±ng máº·t náº¡ [mask]. MÃ´ hÃ¬nh pháº£i dá»± Ä‘oÃ¡n cÃ¡c tá»« gá»‘c táº¡i nhá»¯ng vá»‹ trÃ­ bá»‹ che giáº¥u, dá»±a vÃ o ngá»¯ cáº£nh cá»§a cÃ¡c tá»« xung quanh.

- Next Sentence Prediction (NSP): ÄÃ¢y lÃ  nhiá»‡m vá»¥ mÃ  BERT Ä‘Æ°á»£c huáº¥n luyá»‡n Ä‘á»ƒ xÃ¡c Ä‘á»‹nh xem má»™t cÃ¢u cÃ³ pháº£i lÃ  cÃ¢u tiáº¿p theo cá»§a cÃ¢u trÆ°á»›c Ä‘Ã³ trong vÄƒn báº£n gá»‘c hay khÃ´ng. Nhiá»‡m vá»¥ nÃ y giÃºp mÃ´ hÃ¬nh náº¯m báº¯t má»‘i quan há»‡ giá»¯a cÃ¡c cÃ¢u.

Äá»ƒ thá»±c hiá»‡n tinh chá»‰nh (fine-tuning), mÃ´ hÃ¬nh BERT sáº½ Ä‘Æ°á»£c khá»Ÿi táº¡o vá»›i cÃ¡c tham sá»‘ Ä‘Ã£ Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c, sau Ä‘Ã³ táº¥t cáº£ cÃ¡c tham sá»‘ nÃ y sáº½ Ä‘Æ°á»£c Ä‘iá»u chá»‰nh thÃ´ng qua viá»‡c sá»­ dá»¥ng dá»¯ liá»‡u cÃ³ gÃ¡n nhÃ£n tá»« cÃ¡c tÃ¡c vá»¥ xuÃ´i dÃ²ng (downstream tasks). Má»—i tÃ¡c vá»¥ xuÃ´i dÃ²ng cÃ³ cÃ¡c mÃ´ hÃ¬nh tinh chá»‰nh riÃªng, máº·c dÃ¹ táº¥t cáº£ Ä‘á»u Ä‘Æ°á»£c khá»Ÿi táº¡o tá»« cÃ¹ng má»™t táº­p tham sá»‘ Ä‘Ã£ huáº¥n luyá»‡n trÆ°á»›c. Trong quÃ¡ trÃ¬nh nÃ y, cÃ¡c tham sá»‘ cá»§a mÃ´ hÃ¬nh sáº½ Ä‘Æ°á»£c Ä‘iá»u chá»‰nh thÃ´ng qua viá»‡c huáº¥n luyá»‡n cÃ³ giÃ¡m sÃ¡t trÃªn dá»¯ liá»‡u Ä‘Ã£ Ä‘Æ°á»£c gÃ¡n nhÃ£n cá»¥ thá»ƒ cho nhiá»‡m vá»¥ Ä‘Ã³.

### MÃ´ hÃ¬nh GPT-2 (Generative Pretrained Transformer 2)
MÃ´ hÃ¬nh GPT-2 (Generative Pretrained Transformer 2) lÃ  má»™t trong nhá»¯ng mÃ´ hÃ¬nh ngÃ´n ngá»¯ tiÃªn tiáº¿n Ä‘Æ°á»£c phÃ¡t triá»ƒn bá»Ÿi OpenAI. Ra máº¯t vÃ o nÄƒm 2019, GPT-2 lÃ  phiÃªn báº£n cáº£i tiáº¿n cá»§a mÃ´ hÃ¬nh GPT Ä‘áº§u tiÃªn, vá»›i kháº£ nÄƒng sinh ra vÄƒn báº£n tá»± nhiÃªn má»™t cÃ¡ch mÆ°á»£t mÃ  vÃ  tá»± nhiÃªn hÆ¡n. MÃ´ hÃ¬nh nÃ y dá»±a trÃªn kiáº¿n trÃºc Transformer, cho phÃ©p nÃ³ xá»­ lÃ½ vÃ  táº¡o ra vÄƒn báº£n dá»±a trÃªn bá»‘i cáº£nh trÆ°á»›c Ä‘Ã³ mÃ  khÃ´ng cáº§n thiáº¿t pháº£i cÃ³ dá»¯ liá»‡u Ä‘áº§u vÃ o cá»¥ thá»ƒ.

GPT-2 Ä‘Ã£ Ä‘Æ°á»£c huáº¥n luyá»‡n trÃªn má»™t táº­p dá»¯ liá»‡u khá»•ng lá»“ tá»« Internet, giÃºp nÃ³ cÃ³ kháº£ nÄƒng hiá»ƒu ngá»¯ nghÄ©a, ngá»¯ phÃ¡p, vÃ  phong cÃ¡ch viáº¿t cá»§a con ngÆ°á»i. Vá»›i 1,5 tá»· tham sá»‘, GPT-2 cÃ³ kháº£ nÄƒng táº¡o ra vÄƒn báº£n cÃ³ Ä‘á»™ dÃ i khÃ¡c nhau vÃ  thá»±c hiá»‡n nhiá»u tÃ¡c vá»¥ khÃ¡c nhau, tá»« viáº¿t bÃ i bÃ¡o, tÃ³m táº¯t vÄƒn báº£n, cho Ä‘áº¿n táº¡o ra cÃ¡c cÃ¢u chuyá»‡n hÆ° cáº¥u.

MÃ´ hÃ¬nh GPT-2 hoáº¡t Ä‘á»™ng dá»±a trÃªn cÃ¡c nguyÃªn lÃ½ chÃ­nh sau:

- Kiáº¿n trÃºc Transformer: GPT-2 sá»­ dá»¥ng kiáº¿n trÃºc Transformer, bao gá»“m cÃ¡c thÃ nh pháº§n chÃ­nh nhÆ° Multi-Head Attention vÃ  Feedforward Neural Network. Kiáº¿n trÃºc nÃ y cho phÃ©p mÃ´ hÃ¬nh xá»­ lÃ½ cÃ¡c má»‘i quan há»‡ trong dá»¯ liá»‡u Ä‘áº§u vÃ o má»™t cÃ¡ch hiá»‡u quáº£, táº­p trung vÃ o cÃ¡c pháº§n quan trá»ng cá»§a vÄƒn báº£n mÃ  khÃ´ng phá»¥ thuá»™c vÃ o thá»© tá»± tá»« ngá»¯.

- Pretraining vÃ  Fine-tuning:
    - Pretraining: GPT-2 Ä‘Æ°á»£c huáº¥n luyá»‡n trÃªn má»™t lÆ°á»£ng lá»›n dá»¯ liá»‡u chÆ°a Ä‘Æ°á»£c gÃ¡n nhÃ£n (unsupervised data). MÃ´ hÃ¬nh há»c cÃ¡ch dá»± Ä‘oÃ¡n tá»« tiáº¿p theo trong má»™t cÃ¢u, dá»±a trÃªn cÃ¡c tá»« trÆ°á»›c Ä‘Ã³. QuÃ¡ trÃ¬nh nÃ y giÃºp nÃ³ náº¯m báº¯t cÃ¡c Ä‘áº·c Ä‘iá»ƒm ngá»¯ nghÄ©a vÃ  ngá»¯ phÃ¡p cá»§a ngÃ´n ngá»¯.
    - Fine-tuning: Sau khi pretraining, mÃ´ hÃ¬nh cÃ³ thá»ƒ Ä‘Æ°á»£c tinh chá»‰nh (fine-tuned) trÃªn cÃ¡c tÃ¡c vá»¥ cá»¥ thá»ƒ vá»›i dá»¯ liá»‡u gÃ¡n nhÃ£n Ä‘á»ƒ cáº£i thiá»‡n hiá»‡u suáº¥t.
- CÆ¡ cháº¿ Attention: CÆ¡ cháº¿ Attention cho phÃ©p mÃ´ hÃ¬nh chÃº Ã½ Ä‘áº¿n cÃ¡c tá»« hoáº·c cá»¥m tá»« khÃ¡c nhau trong vÄƒn báº£n, giÃºp xÃ¡c Ä‘á»‹nh táº§m quan trá»ng cá»§a chÃºng trong ngá»¯ cáº£nh. Äiá»u nÃ y cho phÃ©p GPT-2 táº¡o ra cÃ¡c cÃ¢u cÃ³ tÃ­nh liÃªn káº¿t cao vÃ  phÃ¹ há»£p hÆ¡n.

- Sinh vÄƒn báº£n tá»± Ä‘á»™ng: Khi nháº­n Ä‘áº§u vÃ o lÃ  má»™t chuá»—i tá»«, GPT-2 sáº½ sinh ra tá»« tiáº¿p theo báº±ng cÃ¡ch dá»± Ä‘oÃ¡n xÃ¡c suáº¥t cá»§a táº¥t cáº£ cÃ¡c tá»« trong tá»« Ä‘iá»ƒn vÃ  chá»n tá»« cÃ³ xÃ¡c suáº¥t cao nháº¥t. QuÃ¡ trÃ¬nh nÃ y láº·p láº¡i cho Ä‘áº¿n khi mÃ´ hÃ¬nh táº¡o ra má»™t Ä‘oáº¡n vÄƒn báº£n hoÃ n chá»‰nh.

GPT-2 lÃ  má»™t bÆ°á»›c tiáº¿n lá»›n trong lÄ©nh vá»±c xá»­ lÃ½ ngÃ´n ngá»¯ tá»± nhiÃªn, nhá» vÃ o kiáº¿n trÃºc Transformer vÃ  kháº£ nÄƒng sinh vÄƒn báº£n máº¡nh máº½. Sá»± ra Ä‘á»i cá»§a nÃ³ khÃ´ng chá»‰ mang láº¡i nhá»¯ng á»©ng dá»¥ng há»¯u Ã­ch trong nhiá»u lÄ©nh vá»±c mÃ  cÃ²n má»Ÿ ra hÆ°á»›ng nghiÃªn cá»©u má»›i cho cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ trong tÆ°Æ¡ng lai.

### MÃ´ hÃ¬nh RoBERTa (Robustly Optimized BERT)
RoBERTa (Robustly optimized BERT approach) lÃ  má»™t mÃ´ hÃ¬nh ngÃ´n ngá»¯ Ä‘Æ°á»£c phÃ¡t triá»ƒn bá»Ÿi Facebook AI Research, ra máº¯t vÃ o nÄƒm 2019. ÄÃ¢y lÃ  phiÃªn báº£n cáº£i tiáº¿n cá»§a mÃ´ hÃ¬nh BERT (Bidirectional Encoder Representations from Transformers) do Google giá»›i thiá»‡u. RoBERTa Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘á»ƒ tá»‘i Æ°u hÃ³a hiá»‡u suáº¥t cá»§a BERT báº±ng cÃ¡ch Ä‘iá»u chá»‰nh cÃ¡c thÃ nh pháº§n trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n vÃ  Ã¡p dá»¥ng nhá»¯ng cáº£i tiáº¿n ká»¹ thuáº­t.

MÃ´ hÃ¬nh RoBERTa Ä‘Ã£ Ä‘Æ°á»£c huáº¥n luyá»‡n trÃªn má»™t táº­p dá»¯ liá»‡u lá»›n hÆ¡n vÃ  phong phÃº hÆ¡n so vá»›i BERT, giÃºp nÃ³ náº¯m báº¯t cÃ¡c má»‘i quan há»‡ ngá»¯ nghÄ©a vÃ  ngá»¯ phÃ¡p tá»‘t hÆ¡n. Äiá»u nÃ y cho phÃ©p RoBERTa Ä‘áº¡t Ä‘Æ°á»£c hiá»‡u suáº¥t cao hÆ¡n trÃªn nhiá»u tÃ¡c vá»¥ khÃ¡c nhau trong lÄ©nh vá»±c xá»­ lÃ½ ngÃ´n ngá»¯ tá»± nhiÃªn (NLP), bao gá»“m phÃ¢n loáº¡i vÄƒn báº£n, phÃ¢n tÃ­ch cáº£m xÃºc, vÃ  tráº£ lá»i cÃ¢u há»i.

CÆ¡ Cháº¿ Hoáº¡t Äá»™ng cá»§a MÃ´ HÃ¬nh RoBERTa
RoBERTa hoáº¡t Ä‘á»™ng dá»±a trÃªn má»™t sá»‘ nguyÃªn lÃ½ chÃ­nh, tÆ°Æ¡ng tá»± nhÆ° BERT nhÆ°ng cÃ³ nhá»¯ng Ä‘iá»u chá»‰nh Ä‘Ã¡ng chÃº Ã½:

- Kiáº¿n trÃºc Transformer: Giá»‘ng nhÆ° BERT, RoBERTa sá»­ dá»¥ng kiáº¿n trÃºc Transformer, bao gá»“m cÃ¡c lá»›p Encoder. Kiáº¿n trÃºc nÃ y cho phÃ©p mÃ´ hÃ¬nh há»c Ä‘Æ°á»£c cÃ¡c má»‘i quan há»‡ giá»¯a cÃ¡c tá»« trong ngá»¯ cáº£nh, tá»« Ä‘Ã³ cáº£i thiá»‡n kháº£ nÄƒng hiá»ƒu ngÃ´n ngá»¯.

- Huáº¥n luyá»‡n khÃ´ng giÃ¡m sÃ¡t (Unsupervised Training):
    - RoBERTa Ä‘Æ°á»£c huáº¥n luyá»‡n trÃªn má»™t táº­p dá»¯ liá»‡u lá»›n tá»« nhiá»u nguá»“n khÃ¡c nhau (nhÆ° Wikipedia, Common Crawl, vÃ  cÃ¡c trang web khÃ¡c). Táº­p dá»¯ liá»‡u nÃ y khÃ´ng cÃ³ gÃ¡n nhÃ£n, cho phÃ©p mÃ´ hÃ¬nh há»c cÃ¡ch dá»± Ä‘oÃ¡n tá»« tiáº¿p theo trong má»™t chuá»—i mÃ  khÃ´ng cáº§n phá»¥ thuá»™c vÃ o cÃ¡c nhÃ£n cá»¥ thá»ƒ.
- Cáº£i tiáº¿n trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n:
    - TÄƒng kÃ­ch thÆ°á»›c táº­p dá»¯ liá»‡u: RoBERTa Ä‘Æ°á»£c huáº¥n luyá»‡n trÃªn má»™t lÆ°á»£ng dá»¯ liá»‡u lá»›n hÆ¡n so vá»›i BERT, vá»›i má»¥c tiÃªu cáº£i thiá»‡n kháº£ nÄƒng tá»•ng quÃ¡t cá»§a mÃ´ hÃ¬nh.
    - Thay Ä‘á»•i cÃ¡c siÃªu tham sá»‘: CÃ¡c tham sá»‘ nhÆ° kÃ­ch thÆ°á»›c batch, Ä‘á»™ dÃ i chuá»—i, vÃ  sá»‘ lÆ°á»£ng bÆ°á»›c huáº¥n luyá»‡n Ä‘Ã£ Ä‘Æ°á»£c Ä‘iá»u chá»‰nh Ä‘á»ƒ tá»‘i Æ°u hÃ³a hiá»‡u suáº¥t.
    - Loáº¡i bá» nhiá»‡m vá»¥ â€œNext Sentence Predictionâ€ (NSP): KhÃ¡c vá»›i BERT, RoBERTa khÃ´ng sá»­ dá»¥ng nhiá»‡m vá»¥ NSP trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n, mÃ  chá»‰ táº­p trung vÃ o viá»‡c dá»± Ä‘oÃ¡n tá»« tiáº¿p theo. Äiá»u nÃ y giÃºp mÃ´ hÃ¬nh trÃ¡nh Ä‘Æ°á»£c má»™t sá»‘ váº¥n Ä‘á» liÃªn quan Ä‘áº¿n cÃ¡ch mÃ  dá»¯ liá»‡u Ä‘Æ°á»£c gÃ¡n nhÃ£n trong NSP.
- CÆ¡ cháº¿ Attention: RoBERTa cÅ©ng sá»­ dá»¥ng cÆ¡ cháº¿ Attention, cho phÃ©p mÃ´ hÃ¬nh chÃº Ã½ Ä‘áº¿n cÃ¡c tá»« trong ngá»¯ cáº£nh rá»™ng hÆ¡n, tá»« Ä‘Ã³ hiá»ƒu rÃµ hÆ¡n vá» cÃ¡c má»‘i quan há»‡ ngá»¯ nghÄ©a trong vÄƒn báº£n.

- TÃ­nh nÄƒng Fine-tuning: Sau giai Ä‘oáº¡n huáº¥n luyá»‡n khÃ´ng giÃ¡m sÃ¡t, RoBERTa cÃ³ thá»ƒ Ä‘Æ°á»£c tinh chá»‰nh trÃªn cÃ¡c tÃ¡c vá»¥ cá»¥ thá»ƒ vá»›i dá»¯ liá»‡u gÃ¡n nhÃ£n, giÃºp nÃ¢ng cao hiá»‡u suáº¥t trÃªn tá»«ng nhiá»‡m vá»¥ riÃªng láº».

RoBERTa lÃ  má»™t trong nhá»¯ng mÃ´ hÃ¬nh ngÃ´n ngá»¯ máº¡nh máº½ nháº¥t hiá»‡n nay, nhá» vÃ o viá»‡c tá»‘i Æ°u hÃ³a cÃ¡c yáº¿u tá»‘ trong quÃ¡ trÃ¬nh huáº¥n luyá»‡n so vá»›i BERT. Vá»›i kháº£ nÄƒng hiá»ƒu ngÃ´n ngá»¯ tá»± nhiÃªn sÃ¢u sáº¯c hÆ¡n vÃ  tÃ­nh linh hoáº¡t cao, RoBERTa Ä‘Ã£ chá»©ng minh Ä‘Æ°á»£c sá»©c máº¡nh cá»§a nÃ³ trong nhiá»u á»©ng dá»¥ng thá»±c táº¿ trong lÄ©nh vá»±c xá»­ lÃ½ ngÃ´n ngá»¯ tá»± nhiÃªn.


### MÃ´ hÃ¬nh BART
BART (Bidirectional and Auto-Regressive Transformers) lÃ  má»™t mÃ´ hÃ¬nh ngÃ´n ngá»¯ Ä‘Æ°á»£c phÃ¡t triá»ƒn bá»Ÿi Facebook AI Research, ra máº¯t vÃ o nÄƒm 2019. BART káº¿t há»£p cÃ¡c yáº¿u tá»‘ cá»§a hai loáº¡i mÃ´ hÃ¬nh ngÃ´n ngá»¯ chÃ­nh: mÃ´ hÃ¬nh sinh (generative models) vÃ  mÃ´ hÃ¬nh phÃ¢n loáº¡i (discriminative models). MÃ´ hÃ¬nh nÃ y Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘á»ƒ thá»±c hiá»‡n nhiá»u tÃ¡c vá»¥ trong xá»­ lÃ½ ngÃ´n ngá»¯ tá»± nhiÃªn (NLP), bao gá»“m tÃ³m táº¯t vÄƒn báº£n, dá»‹ch mÃ¡y, vÃ  sinh vÄƒn báº£n.

BART sá»­ dá»¥ng kiáº¿n trÃºc Transformer vÃ  cÃ³ kháº£ nÄƒng xá»­ lÃ½ vÄƒn báº£n má»™t cÃ¡ch linh hoáº¡t, cho phÃ©p nÃ³ tÃ¡i cáº¥u trÃºc vÃ  táº¡o ra ná»™i dung má»›i tá»« vÄƒn báº£n Ä‘áº§u vÃ o. Má»™t trong nhá»¯ng Ä‘iá»ƒm ná»•i báº­t cá»§a BART lÃ  kháº£ nÄƒng káº¿t há»£p giá»¯a viá»‡c sá»­ dá»¥ng cÃ¡c phÆ°Æ¡ng phÃ¡p sinh tá»± Ä‘á»™ng vÃ  kháº£ nÄƒng hiá»ƒu ngá»¯ nghÄ©a tá»« vÄƒn báº£n.

CÆ¡ Cháº¿ Hoáº¡t Äá»™ng cá»§a MÃ´ HÃ¬nh BART
BART hoáº¡t Ä‘á»™ng dá»±a trÃªn cÃ¡c nguyÃªn lÃ½ chÃ­nh sau:

- Kiáº¿n trÃºc Transformer: BART dá»±a trÃªn kiáº¿n trÃºc Transformer, bao gá»“m cÃ¡c lá»›p Encoder vÃ  Decoder. Äiá»u nÃ y cho phÃ©p mÃ´ hÃ¬nh há»c vÃ  xá»­ lÃ½ cÃ¡c má»‘i quan há»‡ ngá»¯ nghÄ©a trong vÄƒn báº£n, cÅ©ng nhÆ° táº¡o ra vÄƒn báº£n má»›i.

- CÆ¡ cháº¿ mÃ£ hÃ³a vÃ  giáº£i mÃ£:

    - MÃ£ hÃ³a (Encoding): Trong giai Ä‘oáº¡n nÃ y, BART tiáº¿p nháº­n má»™t Ä‘oáº¡n vÄƒn báº£n Ä‘áº§u vÃ o vÃ  chuyá»ƒn Ä‘á»•i nÃ³ thÃ nh má»™t biá»ƒu diá»…n ngá»¯ nghÄ©a. CÃ¡c lá»›p Encoder sáº½ xá»­ lÃ½ vÄƒn báº£n theo chiá»u hai chiá»u, tá»©c lÃ  xem xÃ©t cáº£ cÃ¡c tá»« trÆ°á»›c vÃ  sau tá»« hiá»‡n táº¡i, giÃºp mÃ´ hÃ¬nh náº¯m báº¯t Ä‘Æ°á»£c ngá»¯ cáº£nh Ä‘áº§y Ä‘á»§.
    - Giáº£i mÃ£ (Decoding): Sau khi mÃ£ hÃ³a, mÃ´ hÃ¬nh sáº½ táº¡o ra vÄƒn báº£n má»›i báº±ng cÃ¡ch sá»­ dá»¥ng cÃ¡c lá»›p Decoder. Giai Ä‘oáº¡n nÃ y cho phÃ©p BART sinh ra vÄƒn báº£n dá»±a trÃªn biá»ƒu diá»…n ngá»¯ nghÄ©a Ä‘Ã£ Ä‘Æ°á»£c mÃ£ hÃ³a.
- Huáº¥n luyá»‡n theo phÆ°Æ¡ng phÃ¡p tá»± giÃ¡m sÃ¡t: BART Ä‘Æ°á»£c huáº¥n luyá»‡n báº±ng cÃ¡ch sá»­ dá»¥ng má»™t táº­p dá»¯ liá»‡u lá»›n vá»›i cÃ¡c nhiá»‡m vá»¥ tá»± giÃ¡m sÃ¡t. Trong quÃ¡ trÃ¬nh nÃ y, mÃ´ hÃ¬nh sáº½ há»c cÃ¡ch phá»¥c há»“i cÃ¡c vÄƒn báº£n bá»‹ lÃ m há»ng (corrupted text) tá»« má»™t vÄƒn báº£n gá»‘c. VÃ­ dá»¥, cÃ¡c tá»« hoáº·c cá»¥m tá»« cÃ³ thá»ƒ bá»‹ xÃ¡o trá»™n, xÃ³a, hoáº·c thay Ä‘á»•i, vÃ  nhiá»‡m vá»¥ cá»§a BART lÃ  khÃ´i phá»¥c láº¡i vÄƒn báº£n gá»‘c.

- Kháº£ nÄƒng Ä‘a nhiá»‡m (Multitasking): BART cÃ³ kháº£ nÄƒng thá»±c hiá»‡n nhiá»u tÃ¡c vá»¥ khÃ¡c nhau, tá»« tÃ³m táº¯t Ä‘áº¿n dá»‹ch mÃ¡y, nhá» vÃ o viá»‡c huáº¥n luyá»‡n trÃªn cÃ¡c nhiá»‡m vá»¥ Ä‘a dáº¡ng. Äiá»u nÃ y cho phÃ©p mÃ´ hÃ¬nh linh hoáº¡t trong viá»‡c xá»­ lÃ½ cÃ¡c loáº¡i vÄƒn báº£n khÃ¡c nhau.

- Káº¿t há»£p giá»¯a phÆ°Æ¡ng phÃ¡p sinh vÃ  phÃ¢n loáº¡i: BART khÃ´ng chá»‰ Ä‘Æ¡n thuáº§n lÃ  má»™t mÃ´ hÃ¬nh sinh (generative model) mÃ  cÃ²n sá»­ dá»¥ng cÃ¡c yáº¿u tá»‘ tá»« mÃ´ hÃ¬nh phÃ¢n loáº¡i, cho phÃ©p nÃ³ khÃ´ng chá»‰ táº¡o ra vÄƒn báº£n má»›i mÃ  cÃ²n hiá»ƒu rÃµ hÆ¡n vá» ngá»¯ nghÄ©a cá»§a vÄƒn báº£n Ä‘áº§u vÃ o.

MÃ´ hÃ¬nh BART lÃ  má»™t bÆ°á»›c tiáº¿n quan trá»ng trong lÄ©nh vá»±c xá»­ lÃ½ ngÃ´n ngá»¯ tá»± nhiÃªn, nhá» vÃ o kháº£ nÄƒng káº¿t há»£p giá»¯a cÃ¡c phÆ°Æ¡ng phÃ¡p sinh vÃ  phÃ¢n loáº¡i. Vá»›i kiáº¿n trÃºc Transformer máº¡nh máº½ vÃ  kháº£ nÄƒng huáº¥n luyá»‡n tá»± giÃ¡m sÃ¡t, BART Ä‘Ã£ chá»©ng minh Ä‘Æ°á»£c hiá»‡u quáº£ cao trÃªn nhiá»u tÃ¡c vá»¥ khÃ¡c nhau, tá»« tÃ³m táº¯t vÄƒn báº£n Ä‘áº¿n dá»‹ch mÃ¡y, lÃ m cho nÃ³ trá»Ÿ thÃ nh má»™t cÃ´ng cá»¥ há»¯u Ã­ch cho nhiá»u á»©ng dá»¥ng trong NLP.


### MÃ´ hÃ¬nh LLaMA
LLaMA (Large Language Model Meta AI) lÃ  má»™t mÃ´ hÃ¬nh ngÃ´n ngá»¯ Ä‘Æ°á»£c phÃ¡t triá»ƒn bá»Ÿi Meta AI (trÆ°á»›c Ä‘Ã¢y lÃ  Facebook AI Research), ra máº¯t vÃ o Ä‘áº§u nÄƒm 2023. MÃ´ hÃ¬nh nÃ y Ä‘Æ°á»£c thiáº¿t káº¿ vá»›i má»¥c tiÃªu cung cáº¥p má»™t ná»n táº£ng máº¡nh máº½ cho cÃ¡c á»©ng dá»¥ng xá»­ lÃ½ ngÃ´n ngá»¯ tá»± nhiÃªn (NLP) vá»›i hiá»‡u suáº¥t cao vÃ  kháº£ nÄƒng linh hoáº¡t. LLaMA nháº±m cáº¡nh tranh vá»›i cÃ¡c mÃ´ hÃ¬nh lá»›n khÃ¡c nhÆ° GPT-3, vÃ  Ä‘Ã£ Ä‘Æ°á»£c phÃ¡t triá»ƒn vá»›i trá»ng tÃ¢m vÃ o viá»‡c tá»‘i Æ°u hÃ³a hiá»‡u suáº¥t vÃ  kháº£ nÄƒng xá»­ lÃ½ ngá»¯ nghÄ©a.

LLaMA bao gá»“m nhiá»u phiÃªn báº£n vá»›i cÃ¡c kÃ­ch thÆ°á»›c khÃ¡c nhau (tá»« 7 triá»‡u Ä‘áº¿n 65 triá»‡u tham sá»‘), giÃºp Ä‘Ã¡p á»©ng Ä‘Æ°á»£c nhu cáº§u Ä‘a dáº¡ng cá»§a ngÆ°á»i dÃ¹ng tá»« cÃ¡c á»©ng dá»¥ng nghiÃªn cá»©u Ä‘áº¿n cÃ¡c sáº£n pháº©m thÆ°Æ¡ng máº¡i. MÃ´ hÃ¬nh nÃ y cÃ³ kháº£ nÄƒng há»c há»i tá»« cÃ¡c dá»¯ liá»‡u Ä‘a dáº¡ng, cho phÃ©p nÃ³ xá»­ lÃ½ nhiá»u tÃ¡c vá»¥ khÃ¡c nhau trong lÄ©nh vá»±c NLP.

CÆ¡ Cháº¿ Hoáº¡t Äá»™ng cá»§a MÃ´ HÃ¬nh LLaMA
LLaMA hoáº¡t Ä‘á»™ng dá»±a trÃªn má»™t sá»‘ nguyÃªn lÃ½ chÃ­nh sau:

- Kiáº¿n trÃºc Transformer: LLaMA Ä‘Æ°á»£c xÃ¢y dá»±ng trÃªn kiáº¿n trÃºc Transformer, cho phÃ©p nÃ³ xá»­ lÃ½ vÃ  táº¡o ra vÄƒn báº£n dá»±a trÃªn má»‘i quan há»‡ giá»¯a cÃ¡c tá»« trong ngá»¯ cáº£nh. Äiá»u nÃ y giÃºp mÃ´ hÃ¬nh náº¯m báº¯t ngá»¯ nghÄ©a vÃ  ngá»¯ phÃ¡p cá»§a ngÃ´n ngá»¯ má»™t cÃ¡ch hiá»‡u quáº£.

- Huáº¥n luyá»‡n tá»± giÃ¡m sÃ¡t (Self-supervised Learning): LLaMA Ä‘Æ°á»£c huáº¥n luyá»‡n trÃªn má»™t lÆ°á»£ng lá»›n dá»¯ liá»‡u vÄƒn báº£n tá»« Internet mÃ  khÃ´ng cÃ³ nhÃ£n. QuÃ¡ trÃ¬nh nÃ y giÃºp mÃ´ hÃ¬nh há»c cÃ¡ch dá»± Ä‘oÃ¡n tá»« tiáº¿p theo trong má»™t chuá»—i, tá»« Ä‘Ã³ hiá»ƒu rÃµ hÆ¡n vá» cÃ¡ch sá»­ dá»¥ng ngÃ´n ngá»¯.

- Tá»‘i Æ°u hÃ³a cho hiá»‡u suáº¥t:

    - LLaMA Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘á»ƒ tá»‘i Æ°u hÃ³a kháº£ nÄƒng sinh vÄƒn báº£n mÃ  váº«n duy trÃ¬ hiá»‡u suáº¥t cao. Äiá»u nÃ y bao gá»“m viá»‡c cáº£i thiá»‡n cÃ¡c thuáº­t toÃ¡n huáº¥n luyá»‡n vÃ  cÃ¡c phÆ°Æ¡ng phÃ¡p Ä‘iá»u chá»‰nh siÃªu tham sá»‘.
    - MÃ´ hÃ¬nh cÅ©ng Ä‘Æ°á»£c tinh chá»‰nh Ä‘á»ƒ giáº£m thiá»ƒu tÃ i nguyÃªn tÃ­nh toÃ¡n cáº§n thiáº¿t, giÃºp nÃ³ hoáº¡t Ä‘á»™ng hiá»‡u quáº£ hÆ¡n trÃªn cÃ¡c pháº§n cá»©ng háº¡n cháº¿.
- Kháº£ nÄƒng Ä‘a nhiá»‡m (Multitasking): LLaMA cÃ³ kháº£ nÄƒng thá»±c hiá»‡n nhiá»u tÃ¡c vá»¥ khÃ¡c nhau trong xá»­ lÃ½ ngÃ´n ngá»¯ tá»± nhiÃªn, bao gá»“m táº¡o vÄƒn báº£n, tÃ³m táº¯t, dá»‹ch mÃ¡y vÃ  tráº£ lá»i cÃ¢u há»i. Äiá»u nÃ y cho phÃ©p mÃ´ hÃ¬nh linh hoáº¡t trong viá»‡c Ä‘Ã¡p á»©ng nhu cáº§u Ä‘a dáº¡ng cá»§a ngÆ°á»i dÃ¹ng.

- Cáº£i tiáº¿n dá»±a trÃªn pháº£n há»“i: LLaMA cÃ³ thá»ƒ Ä‘Æ°á»£c tinh chá»‰nh dá»±a trÃªn pháº£n há»“i tá»« ngÆ°á»i dÃ¹ng vÃ  káº¿t quáº£ thá»±c táº¿ Ä‘á»ƒ cáº£i thiá»‡n hiá»‡u suáº¥t trÃªn cÃ¡c tÃ¡c vá»¥ cá»¥ thá»ƒ.

MÃ´ hÃ¬nh LLaMA Ä‘áº¡i diá»‡n cho má»™t bÆ°á»›c tiáº¿n quan trá»ng trong viá»‡c phÃ¡t triá»ƒn cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ lá»›n, vá»›i má»¥c tiÃªu cung cáº¥p kháº£ nÄƒng xá»­ lÃ½ ngÃ´n ngá»¯ tá»± nhiÃªn máº¡nh máº½ vÃ  hiá»‡u quáº£. Nhá» vÃ o kiáº¿n trÃºc Transformer vÃ  cÃ¡c phÆ°Æ¡ng phÃ¡p huáº¥n luyá»‡n tiÃªn tiáº¿n, LLaMA Ä‘Ã£ cho tháº¥y kháº£ nÄƒng linh hoáº¡t vÃ  hiá»‡u suáº¥t cao trÃªn nhiá»u tÃ¡c vá»¥ khÃ¡c nhau, trá»Ÿ thÃ nh má»™t cÃ´ng cá»¥ há»¯u Ã­ch trong lÄ©nh vá»±c nghiÃªn cá»©u vÃ  á»©ng dá»¥ng NLP.
## B.4. Knowledge Graph (Ä‘á»“ thá»‹ tri thá»©c)
KhÃ¡i niá»‡m "Ä‘á»“ thá»‹ tri thá»©c" Ä‘Æ°á»£c láº§n Ä‘áº§u tiÃªn giá»›i thiá»‡u vÃ o nÄƒm 1972 bá»Ÿi nhÃ  ngÃ´n ngá»¯ há»c ngÆ°á»i Ãšc, Edgar W. Schneider, trong má»™t cuá»™c tháº£o luáº­n vá» viá»‡c xÃ¢y dá»±ng há»‡ thá»‘ng hÆ°á»›ng dáº«n mÃ´-Ä‘un cho cÃ¡c khÃ³a há»c. Tuy nhiÃªn, pháº£i Ä‘áº¿n nÄƒm 2012, khi Google ra máº¯t "Google Knowledge Graph" Ä‘á»ƒ cáº£i thiá»‡n cÃ´ng cá»¥ tÃ¬m kiáº¿m cá»§a mÃ¬nh, khÃ¡i niá»‡m nÃ y má»›i Ä‘Æ°á»£c sá»­ dá»¥ng rá»™ng rÃ£i. Ká»ƒ tá»« Ä‘Ã³, ngÃ y cÃ ng nhiá»u Ä‘á»“ thá»‹ tri thá»©c Ä‘Ã£ Ä‘Æ°á»£c phÃ¡t triá»ƒn vÃ  Ä‘Æ°á»£c Ã¡p dá»¥ng tÃ­ch cá»±c trong nhiá»u lÄ©nh vá»±c khÃ¡c nhau.

Máº·c dÃ¹ Ä‘á»“ thá»‹ tri thá»©c Ä‘Ã£ Ä‘Æ°á»£c nghiÃªn cá»©u má»™t cÃ¡ch sÃ¢u rá»™ng, nhÆ°ng váº«n cÃ²n nhiá»u sá»± khÃ´ng rÃµ rÃ ng trong viá»‡c Ä‘Æ°a ra má»™t Ä‘á»‹nh nghÄ©a chÃ­nh xÃ¡c cho khÃ¡i niá»‡m nÃ y. Nhiá»u ná»— lá»±c Ä‘Ã£ Ä‘Æ°á»£c thá»±c hiá»‡n Ä‘á»ƒ mÃ´ táº£ Ä‘á»“ thá»‹ tri thá»©c. Theo pháº§n lá»›n cÃ¡c nghiÃªn cá»©u, Ä‘á»“ thá»‹ tri thá»©c Ä‘Æ°á»£c coi lÃ  má»™t cáº¥u trÃºc Ä‘Æ°á»£c thiáº¿t káº¿ nháº±m tÃ­ch lÅ©y vÃ  truyá»n Ä‘áº¡t tri thá»©c vá» tháº¿ giá»›i thá»±c, trong Ä‘Ã³ cÃ¡c nÃºt Ä‘áº¡i diá»‡n cho cÃ¡c thá»±c thá»ƒ quan tÃ¢m vÃ  cÃ¡c cáº¡nh biá»ƒu thá»‹ má»‘i quan há»‡ giá»¯a nhá»¯ng thá»±c thá»ƒ nÃ y. Vá» máº·t hÃ¬nh thá»©c, Ä‘á»“ thá»‹ tri thá»©c cÃ³ thá»ƒ Ä‘Æ°á»£c xem nhÆ° má»™t Ä‘á»“ thá»‹ cÃ³ hÆ°á»›ng (G), Ä‘Æ°á»£c Ä‘á»‹nh nghÄ©a lÃ  G = (V, E), trong Ä‘Ã³ V Ä‘áº¡i diá»‡n cho cÃ¡c Ä‘á»‰nh hoáº·c nÃºt tÆ°Æ¡ng á»©ng vá»›i cÃ¡c thá»±c thá»ƒ trong tháº¿ giá»›i thá»±c, vÃ  E Ä‘áº¡i diá»‡n cho cÃ¡c cáº¡nh hoáº·c liÃªn káº¿t thá»ƒ hiá»‡n má»‘i quan há»‡ giá»¯a cÃ¡c thá»±c thá»ƒ nÃ y.

ThÃ´ng thÆ°á»ng, cÃ¡c thá»±c thá»ƒ cÃ¹ng vá»›i má»‘i quan há»‡ cá»§a chÃºng Ä‘Æ°á»£c biá»ƒu diá»…n dÆ°á»›i dáº¡ng bá»™ ba (chá»§ ngá»¯, vá»‹ ngá»¯, vÃ  tÃ¢n ngá»¯) vÃ  Ä‘Æ°á»£c hÃ¬nh dung nhÆ° má»™t Ä‘á»“ thá»‹. Trong Ä‘Ã³, má»—i mÃ u cá»§a hÃ¬nh oval tÆ°Æ¡ng á»©ng vá»›i má»™t kiá»ƒu thá»±c thá»ƒ, vÃ  má»—i hÃ¬nh oval lÃ  má»™t thá»±c thá»ƒ. MÅ©i tÃªn cÃ³ hÆ°á»›ng biá»ƒu diá»…n má»‘i quan há»‡ giá»¯a cÃ¡c thá»±c thá»ƒ.

Má»™t sá»‘ Ä‘á»“ thá»‹ tri thá»©c cho phÃ©p mÃ´ hÃ¬nh hÃ³a cÃ¡c quan há»‡ nhiá»u ngÃ´i (n-ary relation) thay vÃ¬ chá»‰ giá»›i háº¡n á»Ÿ cÃ¡c quan há»‡ hai ngÃ´i (binary relation). Trong trÆ°á»ng há»£p nÃ y, bá»™ ba mÃ´ táº£ quan há»‡ cÃ³ dáº¡ng (r, e1, e2, ...), vá»›i cÃ¡c ğ‘’ lÃ  cÃ¡c thá»±c thá»ƒ tham gia vÃ o má»‘i quan há»‡ ğ‘Ÿ.

Äá»“ thá»‹ tri thá»©c Ä‘Æ°á»£c á»©ng dá»¥ng cho nhiá»u má»¥c Ä‘Ã­ch khÃ¡c nhau, cháº³ng háº¡n nhÆ° tÃ¬m kiáº¿m vÃ  truy váº¥n thÃ´ng tin (vÃ­ dá»¥ nhÆ° Google vÃ  Bing), há»— trá»£ cho cÃ¡c trá»£ lÃ½ áº£o hoáº·c chatbot (nhÆ° ChatGPT), Ä‘Ã³ng vai trÃ² nhÆ° cÆ¡ sá»Ÿ dá»¯ liá»‡u ngá»¯ nghÄ©a (cháº³ng háº¡n nhÆ° Wikidata), vÃ  phÃ¢n tÃ­ch dá»¯ liá»‡u lá»›n (vÃ­ dá»¥ nhÆ° Walmart). NÃ³ cÅ©ng Ä‘Æ°á»£c Ã¡p dá»¥ng rá»™ng rÃ£i trong cÃ¡c lÄ©nh vá»±c chuyÃªn mÃ´n vÃ  ngÃ nh cÃ´ng nghiá»‡p, bao gá»“m y há»c vÃ  chÄƒm sÃ³c sá»©c khá»e, tÃ i chÃ­nh vÃ  ngÃ¢n hÃ ng, IoT, an ninh máº¡ng, truyá»n thÃ´ng vÃ  giáº£i trÃ­, thÆ°Æ¡ng máº¡i Ä‘iá»‡n tá»­ vÃ  bÃ¡n láº», cÅ©ng nhÆ° giÃ¡o dá»¥c vÃ  nhiá»u lÄ©nh vá»±c há»c thuáº­t, khoa há»c, cÃ´ng nghiá»‡p khÃ¡c.

TÃ¹y thuá»™c vÃ o tÃ­nh cháº¥t cá»§a tri thá»©c Ä‘Æ°á»£c lÆ°u trá»¯ trong Ä‘á»“ thá»‹, Ä‘á»“ thá»‹ tri thá»©c cÃ³ thá»ƒ Ä‘Æ°á»£c phÃ¢n loáº¡i thÃ nh nhiá»u loáº¡i khÃ¡c nhau. Trong nhiá»u tháº£o luáº­n há»c thuáº­t, Ä‘á»“ thá»‹ tri thá»©c thÆ°á»ng Ä‘Æ°á»£c chia thÃ nh hai loáº¡i chÃ­nh: Ä‘á»“ thá»‹ tri thá»©c chung vÃ  Ä‘á»“ thá»‹ tri thá»©c chuyÃªn biá»‡t. Äá»“ thá»‹ tri thá»©c chung thÆ°á»ng bao quÃ¡t nhiá»u lÄ©nh vá»±c vÃ  thÆ°á»ng bao gá»“m ná»™i dung tá»« cÃ¡c nguá»“n bÃ¡ch khoa toÃ n thÆ°, cháº³ng háº¡n nhÆ° Wikidata, YAGO, vÃ  DBpedia. NgÆ°á»£c láº¡i, Ä‘á»“ thá»‹ tri thá»©c chuyÃªn biá»‡t táº­p trung vÃ o má»™t lÄ©nh vá»±c hoáº·c ngÃ nh cÃ´ng nghiá»‡p cá»¥ thá»ƒ, thÆ°á»ng Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘á»ƒ giáº£i quyáº¿t cÃ¡c váº¥n Ä‘á» hoáº·c nhu cáº§u Ä‘áº·c thÃ¹.

Lingfeng Zhong vÃ  cÃ¡c Ä‘á»“ng nghiá»‡p Ä‘Ã£ tá»•ng há»£p tá»« nhiá»u nguá»“n khÃ¡c nhau vÃ  Ä‘á» xuáº¥t má»™t quy trÃ¬nh tá»•ng quÃ¡t Ä‘á»ƒ xÃ¢y dá»±ng Ä‘á»“ thá»‹ tri thá»©c. Dá»¯ liá»‡u phá»¥c vá»¥ cho viá»‡c xÃ¢y dá»±ng Ä‘á»“ thá»‹ tri thá»©c cÃ³ thá»ƒ Ä‘Æ°á»£c thu tháº­p tá»« nhiá»u nguá»“n khÃ¡c nhau, vá»›i nhiá»u Ä‘á»‹nh dáº¡ng nhÆ° trang web, báº£ng tÃ­nh, hÃ¬nh áº£nh, cÆ¡ sá»Ÿ dá»¯ liá»‡u, vÄƒn báº£n, vÃ  cÃ³ tÃ­nh cháº¥t Ä‘a dáº¡ng nhÆ° cÃ³ cáº¥u trÃºc, bÃ¡n cáº¥u trÃºc, vÃ  phi cáº¥u trÃºc. QuÃ¡ trÃ¬nh nÃ y diá»…n ra qua ba giai Ä‘oáº¡n chÃ­nh: Khai thÃ¡c tri thá»©c (Knowledge Acquisition), Tinh chá»‰nh tri thá»©c (Knowledge Refinement), vÃ  Tiáº¿n hÃ³a tri thá»©c (Knowledge Evolution). Má»—i giai Ä‘oáº¡n nÃ y sáº½ táº¡o ra má»™t káº¿t quáº£ khÃ¡c nhau dÆ°á»›i dáº¡ng Ä‘á»“ thá»‹ tri thá»©c.

HÃ m SoftMax lÃ  má»™t hÃ m toÃ¡n há»c thÆ°á»ng Ä‘Æ°á»£c sá»­ dá»¥ng trong cÃ¡c mÃ´ hÃ¬nh phÃ¢n loáº¡i Ä‘a lá»›p Ä‘á»ƒ chuyá»ƒn Ä‘á»•i má»™t vector thÃ nh xÃ¡c suáº¥t. HÃ m nÃ y nháº­n vÃ o má»™t vector cÃ¡c giÃ¡ trá»‹ thá»±c vÃ  xuáº¥t ra má»™t vector cÃ³ cÃ¹ng kÃ­ch thÆ°á»›c, trong Ä‘Ã³ má»—i giÃ¡ trá»‹ lÃ  xÃ¡c suáº¥t cá»§a tá»«ng lá»›p. HÃ m Log SoftMax, máº·t khÃ¡c, lÃ  logarit tá»± nhiÃªn cá»§a giÃ¡ trá»‹ Ä‘áº§u ra cá»§a hÃ m SoftMax. Viá»‡c sá»­ dá»¥ng Log SoftMax giÃºp cáº£i thiá»‡n Ä‘á»™ á»•n Ä‘á»‹nh sá»‘ há»c vÃ  hiá»‡u suáº¥t tÃ­nh toÃ¡n, Ä‘áº·c biá»‡t lÃ  trong cÃ¡c bÃ i toÃ¡n cÃ³ nhiá»u lá»›p.
### Quy trÃ¬nh chung Ä‘á»ƒ xÃ¢y dá»±ng Ä‘á»“ thá»‹ tri thá»©c
Dá»¯ liá»‡u Ä‘á»ƒ xÃ¢y dá»±ng Ä‘á»“ thá»‹ tri thá»©c cÃ³ thá»ƒ Ä‘Æ°á»£c thu tháº­p tá»« nhiá»u nguá»“n khÃ¡c nhau vÃ  á»Ÿ nhiá»u Ä‘á»‹nh dáº¡ng khÃ¡c nhau, cháº³ng háº¡n nhÆ° trang web, báº£ng tÃ­nh, hÃ¬nh áº£nh, cÆ¡ sá»Ÿ dá»¯ liá»‡u vÃ  vÄƒn báº£n. Nhá»¯ng dá»¯ liá»‡u nÃ y cÃ³ thá»ƒ mang tÃ­nh cháº¥t khÃ¡c nhau, bao gá»“m cÃ³ cáº¥u trÃºc, bÃ¡n cáº¥u trÃºc vÃ  phi cáº¥u trÃºc. QuÃ¡ trÃ¬nh xÃ¢y dá»±ng Ä‘á»“ thá»‹ tri thá»©c tráº£i qua ba giai Ä‘oáº¡n chÃ­nh: Knowledge Acquisition (Thu tháº­p tri thá»©c), Knowledge Refinement (Tinh chá»‰nh tri thá»©c) vÃ  Knowledge Evolution (Tiáº¿n hÃ³a tri thá»©c). Má»—i giai Ä‘oáº¡n sáº½ táº¡o ra káº¿t quáº£ lÃ  Ä‘á»“ thá»‹ tri thá»©c á»Ÿ nhá»¯ng dáº¡ng khÃ¡c nhau.

#### Giai Ä‘oáº¡n "Thu tháº­p tri thá»©c" (Knowledge Acquisition)
BÆ°á»›c nÃ y tiáº¿p nháº­n Ä‘áº§u vÃ o lÃ  dá»¯ liá»‡u Ä‘á»ƒ thá»±c hiá»‡n rÃºt trÃ­ch thÃ´ng tin. Äá»‘i vá»›i dá»¯ liá»‡u phi cáº¥u trÃºc, quÃ¡ trÃ¬nh rÃºt trÃ­ch yÃªu cáº§u Ã¡p dá»¥ng cÃ¡c ká»¹ thuáº­t xá»­ lÃ½ ngÃ´n ngá»¯ tá»± nhiÃªn phá»©c táº¡p vÃ  thÆ°á»ng pháº£i tráº£i qua cÃ¡c bÆ°á»›c sau:

- Entity Discovery: Giai Ä‘oáº¡n xÃ¡c Ä‘á»‹nh cÃ¡c khÃ¡i niá»‡m (concepts) trong dá»¯ liá»‡u cÃ³ thá»ƒ táº¡o thÃ nh cÃ¡c node trong Ä‘á»“ thá»‹ tri thá»©c, bao gá»“m ba bÆ°á»›c nhá» hÆ¡n:

    - Named Entity Recognition: Nháº­n diá»‡n cÃ¡c thá»±c thá»ƒ cÃ³ tÃªn trong dá»¯ liá»‡u vÃ  phÃ¢n loáº¡i chÃºng thÃ nh cÃ¡c kiá»ƒu thá»±c thá»ƒ sÆ¡ bá»™ nhÆ° ngÆ°á»i, tá»• chá»©c, v.v.
    - Entity Typing: XÃ¡c Ä‘á»‹nh kiá»ƒu thá»±c thá»ƒ chi tiáº¿t cho cÃ¡c thá»±c thá»ƒ vá»«a Ä‘Æ°á»£c nháº­n diá»‡n.
    - Entity Linking: LiÃªn káº¿t thá»±c thá»ƒ Ä‘Ã£ khÃ¡m phÃ¡ vá»›i má»™t node trÃªn Ä‘á»“ thá»‹ tri thá»©c (náº¿u khÃ´ng cÃ³, sáº½ táº¡o ra node má»›i).
- Coreference Resolution: ÄÃ¢y lÃ  quÃ¡ trÃ¬nh xÃ¡c Ä‘á»‹nh cÃ¡c Ä‘á» cáº­p (mention) cÃ³ cÃ¹ng tham chiáº¿u Ä‘áº¿n cÃ¡c thá»±c thá»ƒ giá»‘ng nhau hay khÃ´ng.

- Relation Extraction: Giai Ä‘oáº¡n nÃ y liÃªn quan Ä‘áº¿n viá»‡c trÃ­ch xuáº¥t cÃ¡c má»‘i quan há»‡ giá»¯a cÃ¡c thá»±c thá»ƒ cÃ³ trong dá»¯ liá»‡u. ThÃ´ng thÆ°á»ng, nhiá»‡m vá»¥ nÃ y sáº½ phÃ¢n loáº¡i cÃ¡c quan há»‡ vá»«a trÃ­ch xuáº¥t vÃ o cÃ¡c kiá»ƒu quan há»‡ Ä‘Ã£ Ä‘Æ°á»£c Ä‘á»‹nh sáºµn. Tuy nhiÃªn, náº¿u khÃ´ng cÃ³ lÆ°á»£c Ä‘á»“ sáºµn cÃ³, Ä‘Ã¢y sáº½ Ä‘Æ°á»£c coi lÃ  má»™t bÃ i toÃ¡n Open Relation Extraction (rÃºt trÃ­ch quan há»‡ má»Ÿ).

Sau khi hoÃ n thÃ nh giai Ä‘oáº¡n nÃ y, káº¿t quáº£ sáº½ lÃ  Raw Knowledge Graph (KG), tá»©c lÃ  Ä‘á»“ thá»‹ tri thá»©c Ä‘Æ°á»£c táº¡o ra tá»« cÃ¡c bá»™ ba, máº·c dÃ¹ váº«n cÃ²n tá»“n táº¡i sai sÃ³t vÃ  chÆ°a hoÃ n chá»‰nh.

#### Giai Ä‘oáº¡n "Tinh chá»‰nh tri thá»©c" (Knowledge Refinement)
ÄÃ¢y lÃ  bÆ°á»›c nháº±m hoÃ n thiá»‡n Raw Knowledge Graph (KG) Ä‘Æ°á»£c táº¡o ra tá»« giai Ä‘oáº¡n trÆ°á»›c, thÃ´ng qua viá»‡c tham kháº£o má»™t KG Ä‘Ã£ tá»“n táº¡i cho bÃ i toÃ¡n nÃ y hoáº·c nhá»¯ng váº¥n Ä‘á» tÆ°Æ¡ng tá»±. QuÃ¡ trÃ¬nh nÃ y bao gá»“m hai bÆ°á»›c chÃ­nh:

- Knowledge Graph Completion: Giai Ä‘oáº¡n nÃ y táº­p trung vÃ o viá»‡c dáº«n xuáº¥t thÃªm cÃ¡c bá»™ ba má»›i tá»« nhá»¯ng bá»™ ba Ä‘Ã£ Ä‘áº§y Ä‘á»§, nháº±m hoÃ n thiá»‡n cÃ¡c bá»™ ba chÆ°a hoÃ n chá»‰nh.

- Knowledge Fusion: Do yÃªu cáº§u thá»±c tiá»…n luÃ´n thay Ä‘á»•i vÃ  kiáº¿n thá»©c vá» tháº¿ giá»›i bÃªn ngoÃ i khÃ´ng ngá»«ng cáº­p nháº­t, trong háº§u háº¿t cÃ¡c trÆ°á»ng há»£p, Ä‘á»“ thá»‹ tri thá»©c bÃªn ngoÃ i sáº½ Ä‘Æ°á»£c thÃªm vÃ o Ä‘á»ƒ lÃ m phong phÃº cho Ä‘á»“ thá»‹ tri thá»©c hiá»‡n cÃ³. ÄÃ¢y lÃ  hÃ¬nh thá»©c sÃ¡p nháº­p cÃ¡c Ä‘á»“ thá»‹ láº¡i vá»›i nhau.

Káº¿t quáº£ cá»§a giai Ä‘oáº¡n nÃ y lÃ  má»™t Ä‘á»“ thá»‹ tri thá»©c Ä‘Ã£ Ä‘Æ°á»£c tinh chá»‰nh, Ä‘áº£m báº£o tÃ­nh chÃ­nh xÃ¡c vÃ  Ä‘áº§y Ä‘á»§ hÆ¡n so vá»›i phiÃªn báº£n ban Ä‘áº§u.

#### Giai Ä‘oáº¡n "Tiáº¿n hÃ³a tri thá»©c" (Knowledge Evolution)
ÄÃ¢y lÃ  giai Ä‘oáº¡n nÃ¢ng cao hÃ¬nh thá»©c cá»§a Ä‘á»“ thá»‹ tri thá»©c, bao gá»“m cÃ¡c bÆ°á»›c Condition KG Completion, Condition Knowledge Acquisition vÃ  Knowledge Dynamics. Káº¿t quáº£ cá»§a giai Ä‘oáº¡n nÃ y lÃ  má»™t Ä‘á»“ thá»‹ tri thá»©c Ä‘Ã£ Ä‘Æ°á»£c cáº£i tiáº¿n vÃ  nÃ¢ng cao hÆ¡n.

QuÃ¡ trÃ¬nh phÃ¡t triá»ƒn Ä‘á»“ thá»‹ tri thá»©c thÆ°á»ng Ä‘Æ°á»£c chia thÃ nh hai phÆ°Æ¡ng phÃ¡p: tá»« trÃªn xuá»‘ng (top-down) vÃ  tá»« dÆ°á»›i lÃªn (bottom-up). Vá»›i phÆ°Æ¡ng phÃ¡p tá»« trÃªn xuá»‘ng, trÆ°á»›c tiÃªn ngÆ°á»i ta xÃ¢y dá»±ng ontology (hay lÆ°á»£c Ä‘á»“ tri thá»©c) vÃ  sau Ä‘Ã³ trÃ­ch xuáº¥t tri thá»©c dá»±a trÃªn ontology nÃ y. NgÆ°á»£c láº¡i, phÆ°Æ¡ng phÃ¡p tá»« dÆ°á»›i lÃªn báº¯t Ä‘áº§u báº±ng viá»‡c trÃ­ch xuáº¥t tri thá»©c trá»±c tiáº¿p tá»« dá»¯ liá»‡u, rá»“i dá»±a vÃ o Ä‘Ã³ Ä‘á»ƒ xÃ¢y dá»±ng ontology cá»§a Ä‘á»“ thá»‹ tri thá»©c.

Nhiá»u cÃ¡ch thá»©c vÃ  cÃ´ng cá»¥ Ä‘Ã£ Ä‘Æ°á»£c phÃ¡t triá»ƒn Ä‘á»ƒ lÆ°u trá»¯ Ä‘á»“ thá»‹ tri thá»©c. Trong giai Ä‘oáº¡n Ä‘áº§u, cÃ¡c Ä‘á»“ thá»‹ thÆ°á»ng Ä‘Æ°á»£c lÆ°u trá»¯ trong cÃ¡c há»‡ quáº£n trá»‹ cÆ¡ sá»Ÿ dá»¯ liá»‡u quan há»‡ (RDBMS) do tÃ­nh Ä‘Ã¡ng tin cáº­y vÃ  cÃ¡c toÃ¡n tá»­ CRUD (Create, Read, Update, Delete) giÃºp viá»‡c truy xuáº¥t trá»Ÿ nÃªn thuáº­n tiá»‡n. Tuy nhiÃªn, cÃ¡ch thá»©c nÃ y cÃ³ thá»ƒ ráº¥t tá»‘n kÃ©m, Ä‘áº·c biá»‡t lÃ  Ä‘á»‘i vá»›i cÃ¡c Ä‘á»“ thá»‹ tri thá»©c thÆ°a.

CÃ¡c cÆ¡ sá»Ÿ dá»¯ liá»‡u phi quan há»‡ nhÆ° key/value cÅ©ng Ä‘Ã£ Ä‘Æ°á»£c sá»­ dá»¥ng Ä‘á»ƒ lÆ°u trá»¯ Ä‘á»“ thá»‹ tri thá»©c, cháº³ng háº¡n nhÆ° Trinity, Probase, vÃ  CouchDB. Nhá»¯ng xu hÆ°á»›ng há»©a háº¹n cho cÃ´ng nghá»‡ nÃ y lÃ  cÃ¡c cÆ¡ sá»Ÿ dá»¯ liá»‡u tÃ i liá»‡u (document databases) nhÆ° NoSQL vÃ  cÃ¡c cÆ¡ sá»Ÿ dá»¯ liá»‡u Ä‘á»“ thá»‹ nhÆ° Neo4j. NgoÃ i ra, má»™t sá»‘ ngÃ´n ngá»¯ Ä‘Ã£ Ä‘Æ°á»£c phÃ¡t triá»ƒn Ä‘á»ƒ lÆ°u trá»¯ cÃ¡c Ä‘á»“ thá»‹ tri thá»©c, cháº³ng háº¡n nhÆ° RDF vÃ  OWL. Triple store lÃ  má»™t dáº¡ng cÆ¡ sá»Ÿ dá»¯ liá»‡u Ä‘Æ°á»£c thiáº¿t káº¿ Ä‘áº·c biá»‡t Ä‘á»ƒ lÆ°u trá»¯ cÃ¡c bá»™ ba cá»§a Ä‘á»“ thá»‹, phÃ¡t triá»ƒn dá»±a trÃªn nhá»¯ng ngÃ´n ngá»¯ nÃ y.
## B.5. Ká»¹ thuáº­t Prompt (Prompt Engineering)
Ká»¹ thuáº­t Prompt Engineering lÃ  má»™t phÆ°Æ¡ng phÃ¡p tá»‘i Æ°u hÃ³a Ä‘áº§u vÃ o cho cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯, nháº±m cáº£i thiá»‡n káº¿t quáº£ Ä‘áº§u ra. Thay vÃ¬ chá»‰ Ä‘Æ¡n thuáº§n sá»­ dá»¥ng vÄƒn báº£n Ä‘áº§u vÃ o, ngÆ°á»i dÃ¹ng cÃ³ thá»ƒ thiáº¿t káº¿ cÃ¡c cÃ¢u há»i hoáº·c hÆ°á»›ng dáº«n cá»¥ thá»ƒ Ä‘á»ƒ khai thÃ¡c sá»©c máº¡nh cá»§a mÃ´ hÃ¬nh. Ká»¹ thuáº­t nÃ y ráº¥t quan trá»ng trong viá»‡c tá»‘i Æ°u hÃ³a hiá»‡u suáº¥t cá»§a cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ Ä‘Ã£ Ä‘Æ°á»£c huáº¥n luyá»‡n trÆ°á»›c, giÃºp tÄƒng cÆ°á»ng Ä‘á»™ chÃ­nh xÃ¡c vÃ  sá»± phÃ¹ há»£p cá»§a cÃ¡c cÃ¢u tráº£ lá»i mÃ  mÃ´ hÃ¬nh táº¡o ra trong cÃ¡c á»©ng dá»¥ng thá»±c táº¿. BÃªn dÆ°á»›i lÃ  cÃ¡c ká»¹ thuáº­t promt (promt technique) hiá»‡n nay.

### Zero-Shot Prompting
### Few-Shot Prompting
### Chain-of-Thought Prompting
### Self-Consistency
### Generated Knowledge Prompting
### Tree of Thoughts (ToT)
### Others
- Retrieval Augmented Generation (RAG)
- Automatic Reasoning and Tool-use (ART)
- Automatic Reasoning and Tool-use (ART)
- Automatic Prompt Engineer (APE)
- Active-Prompt
- Directional Stimulus Prompting
- PAL (Program-Aided Language Models)
- ReAct Prompting
- Reflexion
- Multimodal CoT Prompting
- GraphPrompts
# C. CÃ¡c cÃ´ng trÃ¬nh liÃªn quan
## C.1. XÃ¢y dá»±ng Knowledge Graph (Ä‘á»“ thá»‹ tri thá»©c)
## C.2. LMs nhÆ° Knowledge-Base (LMs as knowledge bases)
## C.3. TÃ­nh nháº¥t quÃ¡n cá»§a cÃ¡c mÃ´ hÃ¬nh ngÃ´n ngá»¯ (Consistency of LMs)
## C.4. Nháº­n diá»‡n thá»±c thá»ƒ Ä‘Ã£ Ä‘áº·t tÃªn (Named Entity Recognition - NER)
## C.4. RÃºt trÃ­ch quan há»‡ (Relation Extraction)
# D. PhÆ°Æ¡ng phÃ¡p thá»±c hiá»‡n
